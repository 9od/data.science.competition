{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dacon  15회 원자력발전소 상태 판단 모델링 경진대회\n",
    "## 생물학적 수처리 \n",
    "## 2020년 2월 16일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링 코드 작성방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 입상자는 코드 제출 필수. 제출 코드는 예측 결과를 리더보드 점수로 복원할 수 있어야 함\n",
    "\n",
    "2) 코드 제출시 확장자가 R user는 R or .rmd. Python user는 .py or .ipynb\n",
    "\n",
    "3) 코드에 ‘/data’ 데이터 입/출력 경로 포함 제출 or R의 경우 setwd(\" \"), python의 경우 os.chdir을 활용하여 경로 통일\n",
    "\n",
    "4) 전체 프로세스를 일목요연하게 정리하여 주석을 포함하여 하나의 파일로 제출\n",
    "\n",
    "5) 모든 코드는 오류 없이 실행되어야 함(라이브러리 로딩 코드 포함되어야 함).\n",
    "\n",
    "6) 코드와 주석의 인코딩은 모두 UTF-8을 사용하여야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import zipfile ## zip file을 read하는 역할\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import gc\n",
    "import random\n",
    "import joblib # 모델을 저장하고 불러오는 역할\n",
    "\n",
    "## plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "## 모델링에 사용한 라이브러리\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "## 경로 설정\n",
    "path_ = '../0_Data/'\n",
    "soft_train = '../1_Code_train/soft_code/'\n",
    "hard_train = '../1_Code_train/hard_code/'\n",
    "\n",
    "soft_pred = '../2_Code_pred/soft_code/'\n",
    "hard_pred = '../2_Code_pred/hard_code/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 :  Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 60초까지 데이터만 사용\n",
    "- 60초 이후의 데이터는 삭제\n",
    "- train데이터와 label을 붙여주기\n",
    "- zip파일이 아닌 folder를 read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv(path_ + 'train_label.csv')\n",
    "def make_one_file(path_, is_train):\n",
    "    test_list = os.listdir(path_)\n",
    "    li = []\n",
    "    for filename in tqdm(test_list):    \n",
    "        df = pd.read_csv(path_ + filename, index_col=None, header=0)\n",
    "        df['id'] = int(filename.split('.')[0])\n",
    "        \n",
    "        if is_train:\n",
    "            ## train은 label을 붙여주기 \n",
    "            ## 10초 이하는 label을 999라고 임시로 붙여줌\n",
    "            df['label'] = 999\n",
    "            df.loc[10:,'label'] = train_label.loc[train_label['id'] == int(filename.split('.')[0]) , 'label'].values[0] \n",
    "            df = df.loc[:59]    \n",
    "            \n",
    "        li.append(df)\n",
    "\n",
    "    data = pd.concat(li, axis=0, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# train = make_one_file('../0_Data/train/', True)\n",
    "test = make_one_file('../0_Data/test/', False)\n",
    "\n",
    "train.to_pickle('train_df.pickle')\n",
    "test.to_pickle('test_df.pickle')\n",
    "\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 pickle file load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_df.pickle')\n",
    "test = pd.read_pickle('test_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 탐색적 자료분석\n",
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, test 변수별 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist_col(column):\n",
    "    '''plot dist curves for train and test weather data for the given column name'''\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.lineplot(data=train, x='time', y=column, color='green', ax=ax).set_title(column, fontsize=16)\n",
    "    sns.lineplot(data=test, x='time', y=column, color='purple', ax=ax).set_title(column, fontsize=16)\n",
    "    plt.xlabel('time', fontsize=15)\n",
    "    plt.legend(['train', 'test'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist_col('V0000')\n",
    "plot_dist_col('V0001')\n",
    "plot_dist_col('V0012')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일부 변수 correlation 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlations =  train.iloc[:,0:22].corr()\n",
    "# plt.figure(figsize = (20, 12))\n",
    "\n",
    "# Heatmap of correlations\n",
    "sns.clustermap(correlations, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\n",
    "plt.title('Correlation Heatmap');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 변수 선택  Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1-1 Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trim_col = list(set(test.columns) - set(['id', 'time']))\n",
    "for col in tqdm(trim_col):\n",
    "    train[col] = train[col].apply(lambda x: pd.to_numeric( x,  errors='coerce' ) )\n",
    "    test[col] = test[col].apply(lambda x: pd.to_numeric( x,  errors='coerce' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1-2 High correlation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_var = list(train.T.drop_duplicates().reset_index()['index'])\n",
    "test_var = list(test.T.drop_duplicates().reset_index()['index'])\n",
    "\n",
    "var_list = list(set(train_var)|set(test_var))\n",
    "train = train[var_list]\n",
    "\n",
    "var_list.remove('label')\n",
    "test = test[var_list]\n",
    "var_list.append('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1-3 Constant column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_var = []\n",
    "for col in tqdm(var_list):\n",
    "    if train[col].nunique() == 1:\n",
    "        constant_var += [col]\n",
    "\n",
    "var_list.remove('label')        \n",
    "for col in tqdm(var_list):\n",
    "    if test[col].nunique() == 1:\n",
    "        constant_var += [col]\n",
    "var_list.append('label')        \n",
    "        \n",
    "constant_var = list(np.unique(constant_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최종 column 삽입\n",
    "final_var = list(set(var_list) - set(constant_var))\n",
    "train = train[final_var]\n",
    "\n",
    "final_var.remove('label')\n",
    "test = test[final_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train : 상태 A의 label을 999로 했던 것 => 상태B와 동일한 값으로 변경 해줌\n",
    "\n",
    "start = 0\n",
    "if start == 0:\n",
    "    for x in tqdm(range(len(train))):\n",
    "        if train.loc[x:x,'label'].values[0] == 999:\n",
    "            train.loc[x:x, 'label'] = train.loc[x+10:x+10, 'label'].values[0]\n",
    "    start = 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## column 정렬\n",
    "train_col = list(train.columns)\n",
    "train_col.sort()\n",
    "train = train[train_col]\n",
    "\n",
    "\n",
    "test_col = list(test.columns)\n",
    "test_col.sort()\n",
    "test = test[test_col]\n",
    "\n",
    "\n",
    "## 나중에 test 데이터에 사용할 soft var list\n",
    "Soft_var_list = pd.DataFrame(test_col, columns =['var'])\n",
    "Soft_var_list.to_csv('../2_Code_pred/Soft_var_list.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최종 전처리된 파일을 train_soft.pickle, test_soft.pickle로 저장\n",
    "train.to_pickle(soft_train + 'train_soft.pickle')\n",
    "test.to_pickle(soft_train + 'test_soft.pickle')\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_df.pickle')\n",
    "test = pd.read_pickle('test_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2-1 Text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test 중에서 한번이라도 object인 변수 제거\n",
    "train = train.select_dtypes(exclude = 'object')\n",
    "test = test.select_dtypes(exclude = 'object')\n",
    "label = train['label']\n",
    "\n",
    "var_list = sorted(list(set(train.columns) & set(test.columns)))\n",
    "\n",
    "train = train[var_list]\n",
    "test = test[var_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2-2 High correlation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test 각 변수의 평균을 계산하여, 동일하면 중복 컬럼이라고 생각하고 제거.\n",
    "var_list = list(set(list(train.mean().drop_duplicates().index)) & set(list(test.mean().drop_duplicates().index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2-3 Constant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모든 row에서 동일한 값을 갖는 constant 변수 제거\n",
    "constant_var = []\n",
    "for col in tqdm(var_list):\n",
    "    if train[col].nunique() == 1:\n",
    "        constant_var += [col]\n",
    "\n",
    "for col in tqdm(var_list):\n",
    "    if test[col].nunique() == 1:\n",
    "        constant_var += [col]\n",
    "        \n",
    "constant_var = list(np.unique(constant_var))\n",
    "\n",
    "var_list = ['label'] + list(reversed(sorted(list(set(var_list) - set(constant_var)))))\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2-4 NA 있는 변수 제거, 10초 이상 데이터만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_df.pickle')\n",
    "test = pd.read_pickle('test_df.pickle')\n",
    "\n",
    "train_label = train['label']\n",
    "\n",
    "# na가 있는 변수 제거\n",
    "var_list = list(reversed(sorted(list(\n",
    "    set(var_list) - \n",
    "    set(train.columns[train.isna().any()].tolist() \n",
    "        + test.columns[test.isna().any()].tolist())))))\n",
    "\n",
    "# 10초 이상부터 학습에 사용할 것임\n",
    "train = train.loc[(train['time'] >= 10),var_list].reset_index(drop=True)\n",
    "test = test.loc[(test['time'] >= 10),list(set(var_list) - set(['label']))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2-5 categorical / binary 변수 label encoding하여 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "all_value_cnt = all.nunique()\n",
    "\n",
    "# 변수별 nunique를 확인하여, 변수의 타입을 예상.\n",
    "cat_var = sorted(list(all_value_cnt[(all_value_cnt < 11) & (all_value_cnt > 2)].index))\n",
    "bin_var = sorted(list(all_value_cnt[all_value_cnt == 2].index))\n",
    "num_var = sorted(list(set(var_list) - set(cat_var) - set(bin_var) - set(['label','id','time'])))\n",
    "etc_var = ['label','id','time']\n",
    "\n",
    "cat_lbl = pd.DataFrame()\n",
    "for a in (bin_var + cat_var):\n",
    "    cat_lbl[a] = pd.factorize(all[a])[0]\n",
    "\n",
    "dup_cols = {}\n",
    "for i, c1 in enumerate(tqdm(cat_lbl.columns)):\n",
    "    for c2 in cat_lbl.columns[i+1:]:\n",
    "        if c2 not in dup_cols and np.all(cat_lbl[c1] == cat_lbl[c2]):\n",
    "            dup_cols[c2] = c1\n",
    "\n",
    "cat_lbl.drop(dup_cols.keys(), axis = 1, inplace = True)\n",
    "cat_lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = cat_lbl.nunique()\n",
    "cat_var = sorted(list(num[num > 2].index))\n",
    "bin_var = sorted(list(num[num == 2].index))\n",
    "\n",
    "var_type_list = pd.DataFrame({'var' : etc_var + cat_var + bin_var + num_var,\n",
    "              'type' : np.concatenate([np.repeat(['etc'],3),\n",
    "                                       np.repeat(['cat'],len(cat_var)),\n",
    "                                       np.repeat(['bin'],len(bin_var)),\n",
    "                                       np.repeat(['num'],len(num_var))])})\n",
    "\n",
    "all = all[etc_var + num_var + cat_var + bin_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type_list.to_csv('../2_Code_pred/var_type_list.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2-6 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical 변수에 대해 frequency encoding\n",
    "for col in tqdm(cat_var):\n",
    "    temp = all.groupby(col)['id'].count().to_dict()\n",
    "    all[col+'_freq'] = all[col].map(temp)\n",
    "    del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm feature importance top 25 변수\n",
    "# feature engineering 진행 전 초기 모델링에서 산정한 값\n",
    "\n",
    "import_var = ['V3616','V0081','V3324','V3615','V2855','V2859','V1821','V1818','V3098',\n",
    " 'V3461','V4505','V1820','V2861','V2860','V1819','V3432','V2586','V2854',\n",
    " 'V4743','V4824','V3103','V2853','V4495','V2076','V4525']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm Feature importance에서 상위 5개 변수들로 interaction feature 생성\n",
    "for i,col1 in enumerate(import_var[:5]):\n",
    "    for i2 in range(i+1,5):\n",
    "        all[col1+'*'+import_var[:5][i2]] = all[col1] * all[import_var[:5][i2]]\n",
    "        all[col1+'/'+import_var[:5][i2]] = all[col1] / all[import_var[:5][i2]]\n",
    "        all[col1+'+'+import_var[:5][i2]] = all[col1] + all[import_var[:5][i2]]\n",
    "        all[col1+'-'+import_var[:5][i2]] = all[col1] - all[import_var[:5][i2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numeric 변수들을 소수 둘째자리에서 반올림 하고, \n",
    "# time변수와 concat하여 frequency encoding\n",
    "\n",
    "num_var_round = round(all[num_var],2)\n",
    "freq_target = sorted(list(num_var_round.columns[num_var_round.nunique() > 10]))\n",
    "pd.DataFrame({'var' : freq_target}).to_csv('../2_Code_pred/freq_target.csv',index=False)\n",
    "\n",
    "num_var_round = num_var_round[freq_target]\n",
    "idstr= all['time'].astype(str)\n",
    "\n",
    "for a in tqdm(freq_target):\n",
    "    num_var_round[a] = pd.factorize(num_var_round[a])[0]\n",
    "    num_var_round[a] = idstr.str.cat(num_var_round[a].astype(str),sep=',')\n",
    "    temp = num_var_round[a].value_counts().to_dict()\n",
    "    num_var_round[a] = num_var_round[a].map(temp)\n",
    "\n",
    "round_freq_var = [a+'_round_time_freq' for a in num_var_round]\n",
    "num_var_round.columns = round_freq_var\n",
    "\n",
    "del train,test\n",
    "all = pd.concat([all,num_var_round],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all[~all['label'].isnull()].reset_index(drop=True).drop(columns = ['time'])\n",
    "test = all[all['label'].isnull()].reset_index(drop=True).drop(columns = ['time','label'])\n",
    "\n",
    "del all\n",
    "\n",
    "train = train.dropna()\n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2-7 모든 변수 rolling mean 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 모든 변수에 대해 rolling mean\n",
    "\n",
    "train = train.groupby('id').rolling(window = 5).mean().drop(columns = ['id']).reset_index().drop(columns = ['level_1']).dropna().reset_index(drop=True)\n",
    "train_label = train['label']\n",
    "train_id = train['id']\n",
    "\n",
    "test = test.groupby('id').rolling(window = 5).mean().drop(columns = ['id']).reset_index().drop(columns = ['level_1']).dropna().reset_index(drop=True)\n",
    "test_id = test['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 최종 전처리된 파일을 train_hard.pickle, test_hard.pickle로 저장\n",
    "\n",
    "train.to_pickle(hard_train + 'train_hard.pickle')\n",
    "test.to_pickle(hard_train + 'test_hard.pickle')\n",
    "\n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습, 검증, 저장 : Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_train(x_tr, y_tr, x_vl, y_vl, SEED):\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"boosting\": \"gbdt\",\n",
    "        \"num_leaves\": 40,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"feature_fraction\": 0.85,\n",
    "        \"reg_lambda\": 2,\n",
    "        'seed' : SEED,\n",
    "        \"metric\": \"multiclass\",\n",
    "        \"num_class\" : 198\n",
    "            }\n",
    "    lgb_tr = lgb.Dataset(x_tr, label=y_tr)\n",
    "    lgb_vl = lgb.Dataset(x_vl, label=y_vl)\n",
    "\n",
    "    watchlist_1 = [lgb_tr, lgb_vl]\n",
    "\n",
    "\n",
    "# 테스트용 : lgb_model = lgb.train(params, train_set=lgb_tr, num_boost_round=1, valid_sets=watchlist_1, verbose_eval=1, early_stopping_rounds=100)\n",
    "    lgb_model = lgb.train(params, train_set=lgb_tr, num_boost_round=1000, valid_sets=watchlist_1, verbose_eval=100, early_stopping_rounds=100)\n",
    "    \n",
    "    return lgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_predict(test, model, sub_name, test_id):\n",
    "    \n",
    "    if test_id[0] == 828:\n",
    "        prediction = pd.DataFrame(model.predict(test))\n",
    "        sub = pd.concat([pd.DataFrame(test_id),prediction],axis=1).groupby('id').mean().reset_index()\n",
    "        sub.to_csv(sub_name,index=False)\n",
    "    \n",
    "    else :\n",
    "\n",
    "        test = test[test['time'] > 15]\n",
    "        test_id = test['id']\n",
    "        del test['id']\n",
    "        pred = model.predict(test, num_iteration = model.best_iteration) \n",
    "\n",
    "        submission = pd.DataFrame(data=pred)\n",
    "        submission.index = test_id\n",
    "        submission.index.name = 'id'\n",
    "        submission = submission.sort_index()\n",
    "        submission = submission.groupby('id').mean()\n",
    "\n",
    "        submission.to_csv(sub_name, index=True) #제출 파일 만들기\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train(x_tr, y_tr, x_vl, y_vl, SEED):\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"eta\": 0.0123,\n",
    "        \"max_depth\": 3,\n",
    "        \"eval_metric\": 'mlogloss',\n",
    "        \"num_class\" : 198,\n",
    "        \"seed\" : SEED,\n",
    "        'tree_method' : 'gpu_hist', ## gpu 미사용 환경시 주석 처리 바람\n",
    "        'colsample_bytree' : 0.85  ,\n",
    "        'lambda' : 3,\n",
    "        'alpha' : 4    \n",
    "            }\n",
    "\n",
    "    xgb_tr = xgb.DMatrix(x_tr, label=y_tr)\n",
    "    xgb_vl = xgb.DMatrix(x_vl, label=y_vl)\n",
    "\n",
    "    watchlist_1 = [(xgb_tr, 'train'), (xgb_vl, 'valid')]\n",
    "\n",
    "    xgb_model = xgb.train(params, xgb_tr, 1500, watchlist_1, early_stopping_rounds=50, verbose_eval=50)\n",
    "#  테스트용   xgb_model = xgb.train(params, xgb_tr, 1, watchlist_1, early_stopping_rounds=1, verbose_eval=50)    \n",
    "    return xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_predict(test, model, sub_name, test_id):\n",
    "    \n",
    "    \n",
    "    if test_id[0] == 828:\n",
    "        prediction = pd.DataFrame(model.predict(xgb.DMatrix(test)))\n",
    "        sub = pd.concat([pd.DataFrame(test_id),prediction],axis=1).groupby('id').mean().reset_index()\n",
    "        sub.to_csv(sub_name,index=False)   \n",
    "        \n",
    "    else : \n",
    "        test = test[test['time'] > 15]\n",
    "        test_id = test['id']\n",
    "        del test['id']\n",
    "        pred = model.predict(xgb.DMatrix(test))\n",
    "\n",
    "        submission = pd.DataFrame(data=pred)\n",
    "        submission.index = test_id\n",
    "        submission.index.name = 'id'\n",
    "        submission = submission.sort_index()\n",
    "        submission = submission.groupby('id').mean()\n",
    "        \n",
    "        submission.to_csv(sub_name, index=True) #제출 파일 만들기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stacking(dir_):\n",
    "    all_subs = os.listdir(dir_)\n",
    "    num_subs = len(all_subs)\n",
    "    \n",
    "    empty = pd.DataFrame(columns = ['id'] + [str(a) for a in range(198)],\n",
    "                         data = np.zeros((720,199)))\n",
    "\n",
    "    for subs in all_subs:\n",
    "        if 'half' in subs:\n",
    "            temp = pd.read_csv(dir_+subs)\n",
    "            empty = empty + temp\n",
    "\n",
    "        else:\n",
    "            print(subs,'는 대상 파일이 아닙니다.')\n",
    "            num_subs = num_subs - 1\n",
    "\n",
    "    final_subs = empty/num_subs\n",
    "\n",
    "    final_subs['id'] = final_subs['id'].astype(int)\n",
    "    \n",
    "    return(final_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_half1_tr_vl_split(train_df, num, seed):    \n",
    "    '''\n",
    "    train / validation split 함수\n",
    "    train 에 모든 label이 최소 한번은 등장 & train과 validation의 id는 겹치지 않도록 split함.\n",
    "    \n",
    "    train_df : train 데이터\n",
    "    num : label 당 몇개의 id를 뽑을 것이냐.\n",
    "    seed = random seed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    valid_id = []\n",
    "    vc = train_df[['id','label']].drop_duplicates()['label'].value_counts()\n",
    "    temp = list(vc[vc > num].index)\n",
    "    for a in temp:\n",
    "        id_list = list(train_df[train_df['label'] == a]['id'])\n",
    "        valid_id += random.sample(id_list,num)\n",
    "    \n",
    "    train_id = list(set(train_df['id']) - set(valid_id))\n",
    "    \n",
    "    x_tr_ = train_df[train_df['id'].isin(train_id)]\n",
    "    y_tr_ = x_tr_['label']\n",
    "    del x_tr_['label'], x_tr_['id']    \n",
    "\n",
    "    x_vl_ = train_df[~train_df['id'].isin(train_id)]\n",
    "    y_vl_ = x_vl_['label']\n",
    "    del x_vl_['label'] \n",
    "    \n",
    "    return x_tr_, y_tr_, x_vl_, y_vl_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_half2_tr_vl_split(train_df, before_x_vl, num, seed):    \n",
    "    '''\n",
    "    train / validation split 함수\n",
    "    train 에 모든 label이 최소 한번은 등장 & train과 validation의 id는 겹치지 않도록 split함.\n",
    "    \n",
    "    train_df : train 데이터\n",
    "    num : label 당 몇개의 id를 뽑을 것이냐.\n",
    "    seed = random seed\n",
    "    \n",
    "    before_x_vl : half1에서 train에 사용했던 id list\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    valid_id = []\n",
    "    vc = train_df[['id','label']].drop_duplicates()['label'].value_counts()\n",
    "    temp = list(vc[vc > num].index)\n",
    "    for a in temp:\n",
    "        id_list = list(train[train['label'] == a]['id'])    \n",
    "        sample = list(random.sample(id_list,3))\n",
    "        while set(sample) & set(before_x_vl):\n",
    "            ## 겹치는 게 있으면 true, 없으면 false\n",
    "            sample = list(random.sample(id_list,3))\n",
    "        ## 겹치는게 없어서 탈출하면\n",
    "        valid_id += sample\n",
    "    \n",
    "    train_id = list(set(train_df['id']) - set(valid_id))\n",
    "    \n",
    "    x_tr_ = train_df[train_df['id'].isin(train_id)]\n",
    "    y_tr_ = x_tr_['label']\n",
    "    del x_tr_['label'], x_tr_['id']    \n",
    "\n",
    "    x_vl_ = train_df[~train_df['id'].isin(train_id)]\n",
    "    y_vl_ = x_vl_['label']\n",
    "    del x_vl_['label'] ,x_vl_['id']    \n",
    "    \n",
    "    return x_tr_, y_tr_, x_vl_, y_vl_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_half1_tr_vl_split(train_df, num, seed):    \n",
    "    '''\n",
    "    train / validation split 함수\n",
    "    train 에 모든 label이 최소 한번은 등장 & train과 validation의 id는 겹치지 않도록 split함.\n",
    "    \n",
    "    train_df : train 데이터\n",
    "    num : label 당 몇개의 id를 뽑을 것이냐.\n",
    "    seed = random seed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    train_label = train_df['label']\n",
    "    \n",
    "    valid_id = []\n",
    "    vc = train[['id','label']].drop_duplicates()['label'].value_counts()\n",
    "    temp = list(vc[vc > num].index)\n",
    "    for a in temp:\n",
    "        id_list = list(train_df[train_df['label'] == a]['id'])\n",
    "        valid_id += random.sample(id_list,num)\n",
    "    \n",
    "    train_id = list(set(train_df['id']) - set(valid_id))\n",
    "    \n",
    "    x_tr_ = train[train['id'].isin(train_id)]\n",
    "    y_tr_ = train_label[train['id'].isin(train_id)]\n",
    "\n",
    "    x_vl_ = train[~train['id'].isin(train_id)]\n",
    "    y_vl_ = train_label[~train['id'].isin(train_id)]\n",
    "    \n",
    "    return x_tr_, y_tr_, x_vl_, y_vl_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_half2_tr_vl_split(train_df, num, seed):    \n",
    "    '''\n",
    "    train / validation split 함수\n",
    "    train 에 모든 label이 최소 한번은 등장 & train과 validation의 id는 겹치지 않도록 split함.\n",
    "    \n",
    "    train_df : train 데이터\n",
    "    num : label 당 몇개의 id를 뽑을 것이냐.\n",
    "    seed = random seed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    train_label = train_df['label']\n",
    "    \n",
    "    valid_id = []\n",
    "    vc = train_df[['id','label']].drop_duplicates()['label'].value_counts()\n",
    "    temp = list(vc[vc > num].index)\n",
    "    for a in temp:\n",
    "        id_list = list(train[train['label'] == a]['id'])    \n",
    "        sample = list(random.sample(id_list,3))\n",
    "        while set(sample) & set(before_x_vl):\n",
    "            ## 겹치는 게 있으면 true, 없으면 false\n",
    "            sample = list(random.sample(id_list,3))\n",
    "        ## 겹치는게 없어서 탈출하면\n",
    "        valid_id += sample\n",
    "    \n",
    "    train_id = list(set(train_df['id']) - set(valid_id))\n",
    "    \n",
    "    x_tr_ = train[train['id'].isin(train_id)]\n",
    "    y_tr_ = train_label[train['id'].isin(train_id)]\n",
    "\n",
    "    x_vl_ = train[~train['id'].isin(train_id)]\n",
    "    y_vl_ = train_label[~train['id'].isin(train_id)]\n",
    "\n",
    "    \n",
    "    return x_tr_, y_tr_, x_vl_, y_vl_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft 파일\n",
    "### LGBM\n",
    "- half1 seed : 2014 (시간 부족으로 seed 1995, 2020은 모델 학습을 하지 못함)\n",
    "- half2 seed : 1995, 2014, 2020\n",
    "\n",
    "### XGB\n",
    "- half1 seed : 1995, 2014 \n",
    "- half2 seed : 1995, 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(soft_train + 'train_soft.pickle')\n",
    "test = pd.read_pickle(soft_train + 'test_soft.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed 1995, half1\n",
    "- xgb (lgbm은 시간 부족으로 만들지 못함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_vl, y_vl = soft_half1_tr_vl_split(train, 3, seed = 1995)\n",
    "\n",
    "print('train shape :',x_tr.shape)\n",
    "print('validation shape :',x_vl.shape)\n",
    "\n",
    "before_x_vl = list(x_vl['id'].unique())\n",
    "\n",
    "del x_vl['id']  \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgb train\n",
    "xgb_half1_1995 = xgb_train(x_tr, y_tr, x_vl, y_vl, 1995)\n",
    "joblib.dump(xgb_half1_1995, soft_pred+'xgb/xgb_half1_1995.pkl')\n",
    "\n",
    "## xgb predict\n",
    "xgb_predict(test, xgb_half1_1995, soft_train+\"xgb/xgb_half1_1995.csv\",[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed 1995, half2\n",
    "- lgbm, xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_vl, y_vl = soft_half2_tr_vl_split(train, before_x_vl, 3, seed = 1995)\n",
    "\n",
    "print('train shape :',x_tr.shape)\n",
    "print('validation shape :',x_vl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgb train\n",
    "xgb_half2_1995 = xgb_train(x_tr, y_tr, x_vl, y_vl, 1995)\n",
    "joblib.dump(xgb_half2_1995, soft_pred+'xgb/xgb_half2_1995.pkl')\n",
    "\n",
    "## xgb predict\n",
    "xgb_predict(test, xgb_half2_1995, soft_train+\"xgb/xgb_half2_1995.csv\",[0]) \n",
    "\n",
    "## lgbm train\n",
    "lgb_half2_1995 = lgbm_train(x_tr, y_tr, x_vl, y_vl, 1995)\n",
    "joblib.dump(lgb_half2_1995, soft_pred+'lgbm/lgb_half2_1995.pkl')\n",
    "\n",
    "## lgbm predict\n",
    "lgbm_predict(test,lgb_half2_1995, soft_train+\"lgbm/lgb_half2_1995.csv\",[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed 2014, half1\n",
    "- lgbm, xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_vl, y_vl = soft_half1_tr_vl_split(train, 3, seed = 2014)\n",
    "\n",
    "print('train shape :',x_tr.shape)\n",
    "print('validation shape :',x_vl.shape)\n",
    "\n",
    "before_x_vl = list(x_vl['id'].unique())\n",
    "\n",
    "del x_vl['id']  \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lgbm train\n",
    "lgb_half1_2014 = lgbm_train(x_tr, y_tr, x_vl, y_vl, 2014)\n",
    "joblib.dump(lgb_half1_2014, soft_pred+'lgbm/lgb_half1_2014.pkl')\n",
    "\n",
    "## lgbm predict\n",
    "lgbm_predict(test,lgb_half1_2014, soft_train+\"lgbm/lgb_half1_2014.csv\", [0]) \n",
    "\n",
    "## xgb train\n",
    "xgb_half1_2014 = xgb_train(x_tr, y_tr, x_vl, y_vl, 2014)\n",
    "joblib.dump(xgb_half1_2014, soft_pred+'xgb/xgb_half1_2014.pkl')\n",
    "\n",
    "## xgb predict\n",
    "xgb_predict(test, xgb_half1_2014, soft_train+\"xgb/xgb_half1_2014.csv\", [0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed 2014, half2\n",
    "- lgbm, xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_vl, y_vl = soft_half2_tr_vl_split(train, before_x_vl, 3, seed = 2014)\n",
    "\n",
    "print('train shape :',x_tr.shape)\n",
    "print('validation shape :',x_vl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lgbm train\n",
    "lgb_half2_2014 = lgbm_train(x_tr, y_tr, x_vl, y_vl, 2014)\n",
    "joblib.dump(lgb_half2_2014, soft_pred+'lgbm/lgb_half2_2014.pkl')\n",
    "\n",
    "## lgbm predict\n",
    "lgbm_predict(test,lgb_half2_2014, soft_train+\"lgbm/lgb_half2_2014.csv\",[0]) \n",
    "\n",
    "## xgb train\n",
    "xgb_half2_2014 = xgb_train(x_tr, y_tr, x_vl, y_vl, 2014)\n",
    "joblib.dump(xgb_half2_2014, soft_pred+'xgb/xgb_half2_2014.pkl')\n",
    "\n",
    "## xgb predict\n",
    "xgb_predict(test, xgb_half2_2014, soft_train+\"xgb/xgb_half2_2014.csv\", [0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seed 2020, half2\n",
    "- lgbm (시간 부족으로 half1은 만들지 못함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr, x_vl, y_vl = soft_half1_tr_vl_split(train, 3, seed = 2020)\n",
    "\n",
    "print('train shape :',x_tr.shape)\n",
    "print('validation shape :',x_vl.shape)\n",
    "\n",
    "before_x_vl = list(x_vl['id'].unique())\n",
    "\n",
    "del x_vl['id']  \n",
    "gc.collect()\n",
    "\n",
    "x_tr, y_tr, x_vl, y_vl = soft_half2_tr_vl_split(train, before_x_vl, 3, seed = 2020)\n",
    "\n",
    "print('train shape :',x_tr.shape)\n",
    "print('validation shape :',x_vl.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lgbm train\n",
    "lgb_half2_2020 = lgbm_train(x_tr, y_tr, x_vl, y_vl, 2020)\n",
    "joblib.dump(lgb_half2_2020, soft_pred+'lgbm/lgb_half2_2020.pkl')\n",
    "\n",
    "## lgbm test\n",
    "lgbm_predict(test,lgb_half2_2020, soft_train+\"lgbm/lgb_half2_2020.csv\",[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard 파일\n",
    "### LGBM MODELING\n",
    "- half1 seed : 1995, 2014, 2018, 2019, 2020 \n",
    "- half2 seed : 1995, 2014, 2018, 2019, 2020 \n",
    "\n",
    "### XGB MODELING\n",
    "- half1 seed : 1995\n",
    "- half2 seed : 1995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(hard_train + 'train_hard.pickle')\n",
    "test = pd.read_pickle(hard_train + 'test_hard.pickle')\n",
    "test_id = test['id']\n",
    "\n",
    "var_model = sorted(list(set(train.columns) & set(test.columns) - set(['id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for seeds in [1995, 2014, 2018, 2019, 2020]:\n",
    "    print('\" SEED :',seeds,'\"\\n')\n",
    "\n",
    "    \n",
    "    # half 1\n",
    "    print('half1 start...')\n",
    "    x_tr, y_tr, x_vl, y_vl = hard_half1_tr_vl_split(train,2,seed = seeds)\n",
    "\n",
    "    print('train shape :',x_tr.shape)\n",
    "    print('validation shape :',x_vl.shape)\n",
    "    print('test shape :', test.shape)\n",
    "\n",
    "    lgb_model = lgbm_train(x_tr[var_model], y_tr, x_vl[var_model], y_vl, seeds)\n",
    "\n",
    "    print('\\nsave model...')\n",
    "    joblib.dump(lgb_model, hard_pred+'lgbm/lgb_half1_'+ str(seeds) + '.pkl')  \n",
    "\n",
    "    print('\\nmake prediction...')\n",
    "    lgbm_predict(test[var_model],lgb_model, hard_train+\"lgbm/lgb_half1_\"+str(seeds) +\".csv\", test_id) \n",
    "    print('-'*40)\n",
    "\n",
    "    # half 2\n",
    "    print('\\nhalf2 start...')\n",
    "    before_x_vl = list(x_vl['id'].unique())\n",
    "\n",
    "    del x_tr, y_tr, x_vl, y_vl\n",
    "    gc.collect()\n",
    "\n",
    "    x_tr, y_tr, x_vl, y_vl = hard_half2_tr_vl_split(train,2,seed = seeds)\n",
    "\n",
    "    print('train shape :',x_tr.shape)\n",
    "    print('validation shape :',x_vl.shape)\n",
    "    print('test shape :', test.shape)\n",
    "\n",
    "    lgb_model = lgbm_train(x_tr[var_model], y_tr, x_vl[var_model], y_vl, seeds)\n",
    "\n",
    "    print('\\nsave model...')\n",
    "    joblib.dump(lgb_model, hard_pred+'lgbm/lgb_half2_'+ str(seeds) + '.pkl')  \n",
    "\n",
    "    print('\\nmake prediction...')\n",
    "    lgbm_predict(test[var_model],lgb_model, hard_train+\"lgbm/lgb_half2_\"+str(seeds) +\".csv\", test_id) \n",
    "    print('-'*40)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seeds in [1995]:\n",
    "    print('\" SEED :',seeds,'\"\\n')\n",
    "\n",
    "    \n",
    "    # half 1\n",
    "    print('half1 start...')\n",
    "    x_tr, y_tr, x_vl, y_vl = hard_half1_tr_vl_split(train,2,seed = seeds)\n",
    "\n",
    "    print('train shape :',x_tr.shape)\n",
    "    print('validation shape :',x_vl.shape)\n",
    "    print('test shape :', test.shape)\n",
    "\n",
    "    xgb_model = xgb_train(x_tr[var_model], y_tr, x_vl[var_model], y_vl, seeds)\n",
    "\n",
    "    print('\\nsave model...')\n",
    "    joblib.dump(xgb_model, hard_pred+'xgb/xgb_half1_'+ str(seeds) + '.pkl')  \n",
    "\n",
    "    print('\\nmake prediction...')\n",
    "    xgb_predict(test[var_model],xgb_model, hard_train+\"xgb/xgb_half1_\"+str(seeds) +\".csv\", test_id) \n",
    "    print('-'*40)\n",
    "\n",
    "    # half 2\n",
    "    print('\\nhalf2 start...')\n",
    "    before_x_vl = list(x_vl['id'].unique())\n",
    "\n",
    "    del x_tr, y_tr, x_vl, y_vl\n",
    "    gc.collect()\n",
    "\n",
    "    x_tr, y_tr, x_vl, y_vl = hard_half2_tr_vl_split(train,2,seed = seeds)\n",
    "\n",
    "    print('train shape :',x_tr.shape)\n",
    "    print('validation shape :',x_vl.shape)\n",
    "    print('test shape :', test.shape)\n",
    "\n",
    "    xgb_model = xgb_train(x_tr[var_model], y_tr, x_vl[var_model], y_vl, seeds)\n",
    "\n",
    "    print('\\nsave model...')\n",
    "    joblib.dump(xgb_model, hard_pred+'xgb/xgb_half2_'+ str(seeds) + '.pkl')  \n",
    "\n",
    "    print('\\nmake prediction...')\n",
    "    xgb_predict(test[var_model],xgb_model, hard_train+\"xgb/xgb_half2_\"+str(seeds) +\".csv\", test_id) \n",
    "    print('-'*40)\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ENSEMBLE\n",
    "\n",
    "### LGBM ENSEMBLE\n",
    "- 모두 동일한 weight로 simple stacking\n",
    "\n",
    "### XGB ENSEMBLE\n",
    "- 모두 동일한 weight로 simple stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOFT LGBM ENSEMBLE\n",
    "- half1은 1개, half2는 3개의 seed가 존재\n",
    "- 1) half2를 동일한 weight로 simple stacking => half2_new\n",
    "- 2) half1과 half2_new를 동일한 weight로 simple stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## half2 불러와서 합치기\n",
    "half_2_1 = pd.read_csv(soft_train + 'lgbm/' + 'lgb_half2_1995.csv')\n",
    "half_2_2 = pd.read_csv(soft_train + 'lgbm/' + 'lgb_half2_2014.csv')\n",
    "half_2_3 = pd.read_csv(soft_train + 'lgbm/' + 'lgb_half2_2020.csv')\n",
    "\n",
    "id_list = half_2_1.iloc[:, 0:1]\n",
    "sub = (half_2_1.iloc[:,1:] + half_2_2.iloc[:,1:] + half_2_3.iloc[:,1:] )/3\n",
    "soft_lgbm_half2 = pd.concat([id_list, sub], axis= 1)\n",
    "soft_lgbm_half2 = soft_lgbm_half2.set_index('id')\n",
    "soft_lgbm_half2.to_csv(soft_train + 'lgbm/' + 'soft_lgbm_half2.csv')\n",
    "\n",
    "## half1 + half2 합치기\n",
    "half_1 = pd.read_csv(soft_train + 'lgbm/' + 'lgb_half1_2014.csv')\n",
    "half_2 = pd.read_csv(soft_train + 'lgbm/' + 'soft_lgbm_half2.csv')\n",
    "\n",
    "id_list = half_1.iloc[:, 0:1]\n",
    "sub = (half_1.iloc[:,1:] + half_2.iloc[:,1:])/2\n",
    "soft_lgbm_submission = pd.concat([id_list, sub], axis= 1)\n",
    "soft_lgbm_submission = soft_lgbm_submission.set_index('id')\n",
    "\n",
    "soft_lgbm_submission.to_csv(soft_train+'lgbm/lgbm_final_subsmission.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOFT XGB ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_xgb_submission = simple_stacking(dir_ = soft_train + 'xgb/')\n",
    "soft_xgb_submission.to_csv(soft_train+'xgb/xgb_final_subsmission.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HARD LGBM ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_lgbm_submission = simple_stacking(dir_ = hard_train + 'lgbm/')\n",
    "hard_lgbm_submission.to_csv(hard_train+'lgbm/lgbm_final_subsmission.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HARD XGB ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_xgb_submission = simple_stacking(dir_ = hard_train + 'xgb/')\n",
    "hard_xgb_submission.to_csv(hard_train+'xgb/xgb_final_subsmission.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_xgb = pd.read_csv(soft_train+'xgb/xgb_final_subsmission.csv')\n",
    "hard_xgb = pd.read_csv(hard_train+'xgb/xgb_final_subsmission.csv')\n",
    "\n",
    "soft_lgbm = pd.read_csv(soft_train+'lgbm/lgbm_final_subsmission.csv')\n",
    "hard_lgbm = pd.read_csv(hard_train+'lgbm/lgbm_final_subsmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = (0.35*hard_lgbm + 0.35*soft_lgbm + 0.15*hard_xgb + 0.15*soft_xgb)\n",
    "final_submission['id'] = final_submission['id'].astype(int)\n",
    "\n",
    "final_submission.to_csv('../1_Code_train/submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 패키지 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import geopy.distance\n",
    "\n",
    "\n",
    "import  matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from random import sample\n",
    "seed_list = list(range(10000))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\data\\\\raw')\n",
    "\n",
    "TODAY = str(datetime.now().year)+str(datetime.now().month)+str(datetime.now().day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\data')\n",
    "sub = pd.read_csv('submission_sample.csv')\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\code\\\\experiment')\n",
    "experiment_db = pd.read_csv('experiment_DB.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### column 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column명 재지정\n",
    "\n",
    "train.columns  = ['id', 'date', 'bus_route_id', 'in_out', 'station_code', 'station_name',\n",
    " 'latitude', 'longitude', 'ride_6_7', 'ride_7_8', 'ride_8_9',\n",
    " 'ride_9_10', 'ride_10_11', 'ride_11_12', 'takeoff_6_7', 'takeoff_7_8',\n",
    " 'takeoff_8_9', 'takeoff_9_10', 'takeoff_10_11', 'takeoff_11_12',\n",
    " 'ride_18_20']\n",
    "\n",
    "test.columns = ['id', 'date', 'bus_route_id', 'in_out', 'station_code', 'station_name',\n",
    " 'latitude', 'longitude', 'ride_6_7', 'ride_7_8', 'ride_8_9',\n",
    " 'ride_9_10', 'ride_10_11', 'ride_11_12', 'takeoff_6_7', 'takeoff_7_8',\n",
    " 'takeoff_8_9', 'takeoff_9_10', 'takeoff_10_11', 'takeoff_11_12'] \n",
    "\n",
    "# 초기 feature 지정하고 feature generation 할 때 마다 added 리스트에 추가\n",
    "\n",
    "added = []\n",
    "input_var=['in_out','latitude','longitude','ride_6_7', 'ride_7_8', 'ride_8_9', \n",
    "           'ride_9_10','ride_10_11', 'ride_11_12',\n",
    "           'takeoff_6_7', 'takeoff_7_8', 'takeoff_8_9','takeoff_9_10', \n",
    "           'takeoff_10_11', 'takeoff_11_12']\n",
    "target=['ride_18_20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['in_out'] = train['in_out'].map({'시내':0,'시외':1})\n",
    "test['in_out'] = test['in_out'].map({'시내':0,'시외':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요일 변수 추가\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['weekday_var'] = train['date'].dt.weekday\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['weekday_var'] = test['date'].dt.weekday\n",
    "\n",
    "added += ['weekday_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평일, 주말, 공휴일에 대한 feature generation\n",
    "\n",
    "holi = ['2019-09-12','2019-09-13','2019-09-14', '2019-10-03','2019-10-09']\n",
    "wkend = ['2019-09-01','2019-09-07','2019-09-08','2019-09-14','2019-09-15',\n",
    "         '2019-09-21','2019-09-22','2019-09-28','2019-09-29',\n",
    "        '2019-10-05','2019-10-06','2019-10-12','2019-10-13']\n",
    "workday = sorted(list(set(pd.concat([train.date,test.date],axis=0).astype('str').unique()) - set(holi+wkend)))\n",
    "\n",
    "train['day_type'] =  np.where(train.date.isin(holi),1, \n",
    "                            np.where(train.date.isin(wkend),2,3))\n",
    "test['day_type'] =  np.where(test.date.isin(holi),1, \n",
    "                            np.where(test.date.isin(wkend),2,3))\n",
    "\n",
    "added += ['day_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coordinates feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제주 시의 중앙과 서귀포 시의 중앙과 각 정류소의 거리를 구함.\n",
    "\n",
    "coords_jejusi = (33.500770, 126.522761) #제주시의 위도 경도\n",
    "coords_seoquipo = (33.259429, 126.558217) #서귀포시의 위도 경도\n",
    "\n",
    "train['dis_jejusi'] = [geopy.distance.vincenty((train['latitude'].iloc[i],train['longitude'].iloc[i]), coords_jejusi).km for i in range(len(train))]\n",
    "train['dis_seoquipo'] = [geopy.distance.vincenty((train['latitude'].iloc[i],train['longitude'].iloc[i]), coords_seoquipo).km for i in range(len(train))]\n",
    "\n",
    "test['dis_jejusi'] = [geopy.distance.vincenty((test['latitude'].iloc[i],test['longitude'].iloc[i]), coords_jejusi).km for i in range(len(test))]\n",
    "test['dis_seoquipo'] = [geopy.distance.vincenty((test['latitude'].iloc[i],test['longitude'].iloc[i]), coords_seoquipo).km for i in range(len(test))]\n",
    "\n",
    "added += ['dis_jejusi', 'dis_seoquipo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이건 지울 수도 있음\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\data')\n",
    "\n",
    "# 공공데이터 활용하여 2015년 9월, 10월 의 제주도 좌표별 18~20시 유동인구를 구하여 가장 많은 곳을 feature generation\n",
    "move_18_20 = pd.read_pickle('move_18_20.pkl')\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "remove_outlier =\\\n",
    "move_18_20[~((move_18_20.x < 126.46) | ((move_18_20['x'] > 126.56) & (move_18_20['y'] > 33.5)))].reset_index(drop=True)\n",
    "\n",
    "logic1 = ((all.latitude < 33.5) & (all.latitude > 33.47)) & ((all.longitude > 126.47) & (all.longitude < 126.50))\n",
    "logic2 = ((all.latitude < 33.53) & (all.latitude > 33.48)) & ((all.longitude > 126.51) & (all.longitude < 126.54))\n",
    "logic3 = ((all.latitude < 33.26) & (all.latitude > 33.24)) & ((all.longitude > 126.55) & (all.longitude < 126.57))\n",
    "\n",
    "all['high_move'] = np.where(logic1,'1',np.where(logic2,'2',np.where(logic3,'3','0')))\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'high_move' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유동인구가 많았던 곳과의 거리를 구함\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "logic1 = ((remove_outlier.y < 33.5) & (remove_outlier.y > 33.47)) & ((remove_outlier.x > 126.47) & (remove_outlier.x < 126.50))\n",
    "logic2 = ((remove_outlier.y < 33.53) & (remove_outlier.y > 33.48)) & ((remove_outlier.x > 126.51) & (remove_outlier.x < 126.54))\n",
    "logic3 = ((remove_outlier.y < 33.26) & (remove_outlier.y > 33.24)) & ((remove_outlier.x > 126.55) & (remove_outlier.x < 126.57))\n",
    "\n",
    "which1 = (remove_outlier[logic1].y.mean(), remove_outlier[logic1].x.mean())\n",
    "which2 = (remove_outlier[logic2].y.mean(), remove_outlier[logic2].x.mean())\n",
    "which3 = (remove_outlier[logic3].y.mean(), remove_outlier[logic3].x.mean())\n",
    "\n",
    "all['dis_1'] = [geopy.distance.vincenty((all['latitude'].iloc[i],all['longitude'].iloc[i]), which1).km for i in range(len(all))]\n",
    "all['dis_2'] = [geopy.distance.vincenty((all['latitude'].iloc[i],all['longitude'].iloc[i]), which2).km for i in range(len(all))] \n",
    "all['dis_3']  = [geopy.distance.vincenty((all['latitude'].iloc[i],all['longitude'].iloc[i]), which3).km for i in range(len(all))]\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['dis_1','dis_2','dis_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유동인구 데이터를 join\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\data')\n",
    "sep = pd.read_pickle('sep_move.pkl')\n",
    "octo = pd.read_pickle('octo_move.pkl')\n",
    "\n",
    "total = pd.concat([sep,octo],axis=0)\n",
    "total[['x','y']] = round(total[['x','y']],2)\n",
    "temp = total.groupby(['x','y'])['move_18_20'].sum().reset_index().\\\n",
    "rename(columns = {'x' : 'longitude','y':'latitude'})\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "all[['latitude','longitude']] = round(all[['latitude','longitude']],2)\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on=['latitude','longitude'])\n",
    "all['move_18_20'] = all['move_18_20'].fillna(0).astype('int')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['move_18_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날씨 측정소와의 거리에 대한 feature generation\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "jeju=(33.51411, 126.52969) # 제주 측정소 근처\n",
    "gosan=(33.29382, 126.16283) #고산 측정소 근처\n",
    "seongsan=(33.38677, 126.8802) #성산 측정소 근처\n",
    "po=(33.24616, 126.5653) #서귀포 측정소 근처\n",
    "\n",
    "t1 = [geopy.distance.vincenty( (i,j), jeju).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t2 = [geopy.distance.vincenty( (i,j), gosan).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t3 = [geopy.distance.vincenty( (i,j), seongsan).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t4 = [geopy.distance.vincenty( (i,j), po).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "\n",
    "all['dis_jeju']=t1\n",
    "all['dis_gosan']=t2\n",
    "all['dis_seongsan']=t3\n",
    "all['dis_po']=t4\n",
    "\n",
    "all['dist_name'] = all[['dis_jeju','dis_gosan','dis_seongsan','dis_po']].apply(lambda x: np.argmin(x),axis=1).str.slice(4,)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['dis_jeju','dis_gosan','dis_seongsan','dis_po']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018년 제주도 동별 인구 통계를 활용하여 인구수 feature generation하고 aggregation feature 생성\n",
    "# 인구수 feature와 요일 feature를 concat하여 aggregation feature 생성\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\data\\\\raw')\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "pop_refine = pd.read_csv('pop_refine.csv',engine='python')\n",
    "\n",
    "temp = all[['latitude','longitude']].drop_duplicates()\n",
    "\n",
    "temp = pd.DataFrame({\n",
    "    'latitude' : list(np.repeat(temp['latitude'],len(pop_refine))),\n",
    "    'longitude' : list(np.repeat(temp['longitude'],len(pop_refine))),\n",
    "    'index' : list(np.repeat(range(len(temp)),len(pop_refine))),\n",
    "    'latitude_' : list(np.tile(pop_refine['latitude'],len(temp))),\n",
    "    'longitude_' :list(np.tile(pop_refine['longitude'],len(temp))),\n",
    "    'pop' :list(np.tile(pop_refine['pop'],len(temp)))})\n",
    "\n",
    "temp['dist'] = temp.apply(lambda x: geopy.distance.vincenty((x['latitude'],x['longitude']), (x['latitude_'],x['longitude_'])).km,axis=1)\n",
    "\n",
    "temp = pd.merge(temp,temp.groupby(['latitude','longitude'])['dist'].min().reset_index(),how='right',on=['latitude','longitude','dist'])[['latitude','longitude','pop']]\n",
    "\n",
    "temp['pop'] = [int(a.replace(',','')) for a in temp['pop'] ]\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on=['latitude','longitude'])\n",
    "\n",
    "# \n",
    "\n",
    "all['pop_weekday'] = (all['pop'].astype('str') + all['weekday_var'].astype('str')).astype('int')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all,temp\n",
    "\n",
    "added += ['pop','pop_weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 탑승/하차 인원 feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6~12 시의 탑승 인원의 합 / 하차 인원의 합 feature \n",
    "\n",
    "train['ride_6_12'] = train[['ride_6_7','ride_7_8','ride_8_9','ride_9_10','ride_10_11','ride_11_12']].sum(axis=1)\n",
    "test['ride_6_12'] = test[['ride_6_7','ride_7_8','ride_8_9','ride_9_10','ride_10_11','ride_11_12']].sum(axis=1)\n",
    "\n",
    "train['takeoff_6_12'] = train[['takeoff_6_7','takeoff_7_8','takeoff_8_9','takeoff_9_10','takeoff_10_11','takeoff_11_12']].sum(axis=1)\n",
    "test['takeoff_6_12'] = test[['takeoff_6_7','takeoff_7_8','takeoff_8_9','takeoff_9_10','takeoff_10_11','takeoff_11_12']].sum(axis=1)\n",
    "\n",
    "added += ['ride_6_12','takeoff_6_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (ride_6_12 - takeoff_6_12) 계산하여 feature generation\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "all['diff_ride_takeoff'] = all['ride_6_12'] - all['takeoff_6_12']\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['diff_ride_takeoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비선형 패턴을 추가해주기 위해 시간별 탑승, 하차 데이터에 제곱을 해줌\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "temp = np.power(all[['ride_6_7','ride_7_8','ride_8_9','ride_9_10','ride_10_11','ride_11_12','takeoff_6_7','takeoff_7_8','takeoff_8_9','takeoff_9_10','takeoff_10_11','takeoff_11_12']],2)\n",
    "temp.columns = [a+'_power' for a in tuple(temp.columns)]\n",
    "\n",
    "all = pd.concat([all,temp],axis=1)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'power' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target feature와 가장 상관관계가 높았던 ride_6_!2 변수에 대해 rolling mean 변수 추가\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "rmean = all[['date','bus_route_id','station_code','ride_6_12']].sort_values(['bus_route_id','station_code','date']).groupby(['bus_route_id','station_code'])['ride_6_12'].rolling(3).mean().reset_index()[['bus_route_id','station_code','ride_6_12']]\n",
    "rmean['date'] = all[['date','bus_route_id','station_code']].sort_values(['bus_route_id','station_code','date']).reset_index(drop=True)['date']\n",
    "all = pd.merge(all,rmean.rename(columns = {'ride_6_12' : 'r3mean_ride_6_12'}),how='left',on=['bus_route_id','station_code','date'])\n",
    "\n",
    "all['r3mean_ride_6_12'] = all[['bus_route_id','station_code','ride_6_12','r3mean_ride_6_12']].groupby(['bus_route_id','station_code']).\\\n",
    "apply(lambda x: x.fillna(x['ride_6_12'].median()))['r3mean_ride_6_12']\n",
    "\n",
    "rmean = all[['date','bus_route_id','station_code','ride_6_12']].sort_values(['bus_route_id','station_code','date']).groupby(['bus_route_id','station_code'])['ride_6_12'].rolling(5).mean().reset_index()[['bus_route_id','station_code','ride_6_12']]\n",
    "rmean['date'] = all[['date','bus_route_id','station_code']].sort_values(['bus_route_id','station_code','date']).reset_index(drop=True)['date']\n",
    "all = pd.merge(all,rmean.rename(columns = {'ride_6_12' : 'r5mean_ride_6_12'}),how='left',on=['bus_route_id','station_code','date'])\n",
    "\n",
    "all['r5mean_ride_6_12'] = all[['bus_route_id','station_code','ride_6_12','r5mean_ride_6_12']].groupby(['bus_route_id','station_code']).\\\n",
    "apply(lambda x: x.fillna(x['ride_6_12'].median()))['r5mean_ride_6_12']\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['r3mean_ride_6_12','r5mean_ride_6_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregation statistic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버스 노선, 정류소 번호, 정류소 이름 을 ride_6_!2 feature를 target으로 encoding\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "for col in ['station_code','station_name','bus_route_id']:\n",
    "    temp = all.groupby([col])['ride_6_12'].agg(['mean','max','min','count']).reset_index().\\\n",
    "        rename(columns = {'mean' : col+'_'+'ride_6_12'+'_'+'mean_morning',\n",
    "                         'max' : col+'_'+'ride_6_12'+'_'+'max_morning',\n",
    "                         'min' : col+'_'+'ride_6_12'+'_'+'min_morning',\n",
    "                         'count' : col+'_'+'ride_6_12'+'_'+'count_morning'})\n",
    "    all = pd.merge(all,temp,how='left',on=col)\n",
    "\n",
    "# 버스 노선, 정류소 번호, 정류소 이름 을 ride_6_!2 feature를 target으로 encoding\n",
    "\n",
    "for col in ['station_code','station_name','bus_route_id']:\n",
    "    temp = all.groupby([col])['takeoff_6_12'].agg(['mean','max','min','count']).reset_index().\\\n",
    "        rename(columns = {'mean' : col+'_'+'takeoff_6_12'+'_'+'mean_morning',\n",
    "                         'max' : col+'_'+'takeoff_6_12'+'_'+'max_morning',\n",
    "                         'min' : col+'_'+'takeoff_6_12'+'_'+'min_morning',\n",
    "                         'count' : col+'_'+'takeoff_6_12'+'_'+'count_morning'})\n",
    "    all = pd.merge(all,temp,how='left',on=col)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if '_morning' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bus_route_id와 in_out 변수를 concat하여 encoding feature generation\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "all['in_out_bus_route_id'] = all['bus_route_id'].astype('str') + all['in_out'].astype('str')\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['id'].count().to_dict()\n",
    "all['inout_bus_route_id_freq'] = all['in_out_bus_route_id'].map(temp)\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['ride_6_12'].agg(['mean','min','max','sum','count']).rename(\n",
    "columns = {\n",
    "    'mean' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'mean',\n",
    "    'min' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'min',\n",
    "    'max' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'max',\n",
    "    'sum' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'sum'\n",
    "})\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on='in_out_bus_route_id')\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['takeoff_6_12'].agg(['mean','min','max','sum']).rename(\n",
    "columns = {\n",
    "    'mean' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'mean',\n",
    "    'min' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'min',\n",
    "    'max' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'max',\n",
    "    'sum' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'sum'\n",
    "})\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on='in_out_bus_route_id')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'inout_' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 버스 승하차 데이터 활용 feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 버스 승하차 장소가 동일한 데이터가 다수 보여, 이 정보를 바탕으로 feature generation\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\data\\\\raw')\n",
    "bus = pd.read_csv('bus_bts.csv')\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "code_which = all[['station_code','latitude','longitude']].drop_duplicates().reset_index(drop=True).rename(columns = {\n",
    "    'station_code' : 'geton_station_code',\n",
    "    'latitude' : 'geton_lat',\n",
    "    'longitude' : 'geton_long'\n",
    "})\n",
    "bus = pd.merge(bus,code_which, how='left',on='geton_station_code')\n",
    "\n",
    "code_which = all[['station_code','latitude','longitude']].drop_duplicates().reset_index(drop=True).rename(columns = {\n",
    "    'station_code' : 'getoff_station_code',\n",
    "    'latitude' : 'getoff_lat',\n",
    "    'longitude' : 'getoff_long'\n",
    "})\n",
    "bus = pd.merge(bus,code_which, how='left',on='getoff_station_code')\n",
    "\n",
    "all = pd.merge(all,pd.DataFrame({'station_code':bus[bus['geton_station_code'] == bus['getoff_station_code']].geton_station_code.unique(),\n",
    "             'same_on_off' : 1}),how='left',on='station_code') \n",
    "\n",
    "all['same_on_off'] = all['same_on_off'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 승객들의 이동 거리 feature를 target으로 aggregation feature generation\n",
    "\n",
    "geton = []\n",
    "getoff = []\n",
    "for aa,bb,cc,dd in zip(bus['geton_lat'],bus['geton_long'],bus['getoff_lat'],bus['getoff_long']):\n",
    "    a = (aa,bb)\n",
    "    b = (cc,dd)\n",
    "    geton += [a]\n",
    "    getoff += [b]\n",
    "    \n",
    "from haversine import haversine\n",
    "\n",
    "dis = []\n",
    "for on,off in zip(geton,getoff):\n",
    "    dis += [haversine(on,off)]\n",
    "    \n",
    "bus['moving_dis'] = dis\n",
    "\n",
    "temp = bus.groupby('bus_route_id')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_bus'] = all['bus_route_id'].map(temp)\n",
    "\n",
    "temp = bus.groupby('geton_station_code')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_geton'] = all['station_code'].map(temp)\n",
    "\n",
    "temp = bus.groupby('getoff_station_code')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_getoff'] = all['station_code'].map(temp)\n",
    "\n",
    "all['moving_dis_per_bus'] =  all['moving_dis_per_bus'].fillna(all['moving_dis_per_bus'].median()) \n",
    "all['moving_dis_per_getoff'] = all['moving_dis_per_getoff'].fillna(all['moving_dis_per_getoff'].median()) \n",
    "all['moving_dis_per_geton'] = all['moving_dis_per_geton'].fillna(all['moving_dis_per_geton'].median()) \n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'moving_dis' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_card_id 의 등장 횟수가 10번 미만이면 제주도에 거주하는 사람이 아니라 여행객이라고 생각\n",
    "# 각 정류소별로 여행객의 수를 target으로 aggregation feature 생성\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),1,0)\n",
    "temp = bus.groupby('geton_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel1'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),1,0)\n",
    "temp = bus.groupby('getoff_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel2'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),0,1)\n",
    "temp = bus.groupby('geton_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel3'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),0,1)\n",
    "temp = bus.groupby('getoff_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel4'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all, bus\n",
    "\n",
    "added += ['travel1','travel2','travel3','travel4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 날씨 feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019년 9월, 10월 06~12 시의 날씨 데이터 기상청에서 가져옴.\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\DS\\\\캐글\\\\제주도\\\\data')\n",
    "rain = pd.read_csv('rain.csv')[['date','dist_name','sum','std']].rename(columns={'sum':'rain_sum',\n",
    "                                              'std':'rain_std',\n",
    "                                              'max' : 'rain_max',\n",
    "                                              'min': 'rain_min'})\n",
    "\n",
    "temper = pd.read_csv('temper.csv')[['date','dist_name','mean','std']].rename(columns = {'mean':'temp_mean',\n",
    "                                                    'std':'temp_std',\n",
    "                                                    'max' : 'temp_max',\n",
    "                                                    'min' : 'temp_min'})\n",
    "\n",
    "train['date'] = train['date'].astype('str')\n",
    "test['date'] = test['date'].astype('str')\n",
    "\n",
    "train = pd.merge(train,rain,how='left',on=['dist_name','date'])\n",
    "train = pd.merge(train,temper,how='left',on=['dist_name','date'])\n",
    "train['temp_mean'] = train['temp_mean']/(train['temp_mean'].max())\n",
    "train['temp_std'] = train['temp_std']/(train['temp_std'].max())\n",
    "\n",
    "test = pd.merge(test,rain,how='left',on=['dist_name','date'])\n",
    "test = pd.merge(test,temper,how='left',on=['dist_name','date'])\n",
    "test['temp_mean'] = test['temp_mean']/(test['temp_mean'].max())\n",
    "test['temp_std'] = test['temp_std']/(test['temp_std'].max())\n",
    "\n",
    "added += ['rain_sum','rain_std','temp_mean','temp_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean encoding 및 최종 feature 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "all['bus_route_id_station_code_concat'] = str(all['station_code']) + str(all['bus_route_id'])\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "del all\n",
    "\n",
    "input_var = input_var + added\n",
    "\n",
    "mean_encoding_col = ['station_code','bus_route_id','station_name','bus_route_id_station_code_concat']\n",
    "\n",
    "for col in mean_encoding_col:\n",
    "    input_var += [col+'_'+'mean_target_encoding']\n",
    "    input_var += [col+'_'+'max_target_encoding']\n",
    "    input_var += [col+'_'+'min_target_encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoding(col,tr,vl,tst):\n",
    "\n",
    "    temp = tr.groupby([col])['ride_18_20'].agg(['mean','max','min']).reset_index().\\\n",
    "    rename(columns = {'mean' : col+'_'+'mean_target_encoding',\n",
    "                     'max' : col+'_'+'max_target_encoding',\n",
    "                     'min' : col+'_'+'min_target_encoding'\n",
    "                     })\n",
    "    tr_ = pd.merge(tr,temp,how='left',on= col)\n",
    "    vl_ = pd.merge(vl,temp,how='left',on= col)\n",
    "    tst_ = pd.merge(tst,temp,how='left',on= col)\n",
    "    \n",
    "    cols = [a for a in tr_.columns if 'target_encoding' in a]\n",
    "    \n",
    "    tr_[cols] = tr_[cols].fillna(0)\n",
    "    vl_[cols] = vl_[cols].fillna(0)\n",
    "    tst_[cols] = tst_[cols].fillna(0)\n",
    "\n",
    "    return tr_, vl_,tst_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation 전략\n",
    "\n",
    "총 네 가지 방법을 사용했음\n",
    "\n",
    "- KFold validation\n",
    "> 가장 일반적인 방법\n",
    "\n",
    "\n",
    "- stratified KFold validation\n",
    "    1. target 변수가 key\n",
    "    > target변수의 분포가 0값이 굉장히 많은 imbalanced 분포였기 때문에 시도했음\n",
    "    2. date가 key\n",
    "    > 공휴일이나, 주말, 또는 태풍 때문에 날짜별로 편차가 존재하였기 때문에 좀 더 robust한 model을 위해 시도했음.\n",
    "    \n",
    "    \n",
    " \n",
    "- timeseries split\n",
    "> 데이터가 time series 데이터여서 시도했음\n",
    "\n",
    "---\n",
    "##### 결론\n",
    "\n",
    "- stratified KFold validation을 date 기준으로 실행한 것이 가장 성능이 좋았음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train[target]\n",
    "\n",
    "feature_imporatnce = pd.DataFrame()\n",
    "\n",
    "NFOLDS = 6\n",
    "random_seed = sample(seed_list,1)\n",
    "\n",
    "stk = StratifiedKFold(n_splits=NFOLDS,random_state = 1995,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "- hyperparameter 조정이 비교적 쉬운 randomForest regressor 단일 모델을 사용\n",
    "\n",
    "\n",
    "- competition metric은 RMSE, 학습시간 단축을 위해 MSE로 학습\n",
    "\n",
    "\n",
    "- modeling 결과\n",
    "\n",
    "    - *평균적인 CV error = 2.32*\n",
    "    \n",
    "    - *public leaderboard score = 2.24612*\n",
    "    \n",
    "    - **final private leaderboard score = 2.22204**\n",
    "\n",
    "\n",
    "- CV error가 떨어지면 public error도 같이 떨어지는 추세를 보였음.\n",
    "> validation setup 이 성공적이었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "2.224053240843318\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "2.246786602744981\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "2.460679881075407\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "2.255226445528015\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 5\n",
      "2.328838485386612\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 6\n",
      "2.4139398581165263\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "cv score:\n",
      "2.3232894232859027\n",
      "1:55:05.248997\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "time = str(start.hour)+'hr'\n",
    "minute = str(start.minute)+'min'\n",
    "\n",
    "cv_train = np.zeros(len(y_train))\n",
    "cv_pred = np.zeros(test.shape[0])\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold_, (tr_index, vl_index) in enumerate(stk.split(train,train['date'])):\n",
    "    print('Fold:', fold_+1)\n",
    "  \n",
    "    x_tr, x_vl = train.iloc[tr_index], train.iloc[vl_index]\n",
    "    y_tr, y_vl = train[target].iloc[tr_index], train[target].iloc[vl_index]\n",
    "    x_tst = test.copy()\n",
    "    \n",
    "    for aaaa in mean_encoding_col:\n",
    "        x_tr,x_vl,x_tst = mean_encoding(aaaa,x_tr,x_vl,x_tst)\n",
    "    \n",
    "    tr = x_tr[input_var]\n",
    "    vl = x_vl[input_var]\n",
    "    tst = x_tst[input_var]\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=random_seed[0],n_estimators=100,criterion='mse')\n",
    "    rf.fit(tr,y_tr)\n",
    "    \n",
    "    feature_imporatnce = pd.concat([feature_imporatnce, pd.DataFrame({'feature':input_var,'importance':rf.feature_importances_})],axis=0)\n",
    "    \n",
    "    pred = rf.predict(vl)\n",
    "    \n",
    "    print(np.sqrt(mean_squared_error(y_vl,pred)))\n",
    "    cv_train[vl_index] += pred\n",
    "    cv_pred += rf.predict(tst)\n",
    "    \n",
    "    print('-'*40+'\\n\\n')\n",
    "    \n",
    "cv_pred /= NFOLDS\n",
    "\n",
    "vl_error = np.sqrt(mean_squared_error(np.array(y_train).flatten(),cv_train))\n",
    "\n",
    "print('cv score:')\n",
    "print(vl_error)\n",
    "\n",
    "sub['18~20_ride'] = cv_pred\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>r5mean_ride_6_12</td>\n",
       "      <td>0.181109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ride_6_12</td>\n",
       "      <td>0.111672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>r3mean_ride_6_12</td>\n",
       "      <td>0.111249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>station_code_max_target_encoding</td>\n",
       "      <td>0.108630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bus_route_id_mean_target_encoding</td>\n",
       "      <td>0.045365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>station_code_mean_target_encoding</td>\n",
       "      <td>0.044615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>station_name_max_target_encoding</td>\n",
       "      <td>0.041117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>station_name_mean_target_encoding</td>\n",
       "      <td>0.039297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>pop_weekday</td>\n",
       "      <td>0.020839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>temp_std</td>\n",
       "      <td>0.018945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bus_route_id_max_target_encoding</td>\n",
       "      <td>0.015629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>weekday_var</td>\n",
       "      <td>0.015291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>diff_ride_takeoff</td>\n",
       "      <td>0.014609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>temp_mean</td>\n",
       "      <td>0.012227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ride_11_12</td>\n",
       "      <td>0.010039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ride_11_12_power</td>\n",
       "      <td>0.009734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>day_type</td>\n",
       "      <td>0.009446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>takeoff_6_12</td>\n",
       "      <td>0.007853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bus_route_id_ride_6_12_max_morning</td>\n",
       "      <td>0.006042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>inout_bus_route_id_ride_6_12_max</td>\n",
       "      <td>0.005807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                feature  importance\n",
       "45                     r5mean_ride_6_12    0.181109\n",
       "52                            ride_6_12    0.111672\n",
       "44                     r3mean_ride_6_12    0.111249\n",
       "61     station_code_max_target_encoding    0.108630\n",
       "1     bus_route_id_mean_target_encoding    0.045365\n",
       "62    station_code_mean_target_encoding    0.044615\n",
       "72     station_name_max_target_encoding    0.041117\n",
       "73    station_name_mean_target_encoding    0.039297\n",
       "43                          pop_weekday    0.020839\n",
       "97                             temp_std    0.018945\n",
       "0      bus_route_id_max_target_encoding    0.015629\n",
       "102                         weekday_var    0.015291\n",
       "15                    diff_ride_takeoff    0.014609\n",
       "96                            temp_mean    0.012227\n",
       "50                           ride_11_12    0.010039\n",
       "51                     ride_11_12_power    0.009734\n",
       "14                             day_type    0.009446\n",
       "87                         takeoff_6_12    0.007853\n",
       "4    bus_route_id_ride_6_12_max_morning    0.006042\n",
       "28     inout_bus_route_id_ride_6_12_max    0.005807"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imporatnce.groupby(['feature'])['importance'].mean().reset_index().sort_values('importance',ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

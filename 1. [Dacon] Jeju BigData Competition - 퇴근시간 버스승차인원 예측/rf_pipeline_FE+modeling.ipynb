{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from random import sample\n",
    "seed_list = list(range(10000))\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data\\\\raw')\n",
    "\n",
    "TODAY = str(datetime.now().year)+str(datetime.now().month)+str(datetime.now().day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data')\n",
    "sub = pd.read_csv('submission_sample.csv')\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\code\\\\experiment')\n",
    "experiment_db = pd.read_csv('experiment_DB.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 명 재지정\n",
    "train.columns  = ['id', 'date', 'bus_route_id', 'in_out', 'station_code', 'station_name',\n",
    " 'latitude', 'longitude', 'ride_6_7', 'ride_7_8', 'ride_8_9',\n",
    " 'ride_9_10', 'ride_10_11', 'ride_11_12', 'takeoff_6_7', 'takeoff_7_8',\n",
    " 'takeoff_8_9', 'takeoff_9_10', 'takeoff_10_11', 'takeoff_11_12',\n",
    " 'ride_18_20']\n",
    "\n",
    "test.columns = ['id', 'date', 'bus_route_id', 'in_out', 'station_code', 'station_name',\n",
    " 'latitude', 'longitude', 'ride_6_7', 'ride_7_8', 'ride_8_9',\n",
    " 'ride_9_10', 'ride_10_11', 'ride_11_12', 'takeoff_6_7', 'takeoff_7_8',\n",
    " 'takeoff_8_9', 'takeoff_9_10', 'takeoff_10_11', 'takeoff_11_12'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['weekday'] = train['date'].dt.weekday\n",
    "train = pd.get_dummies(train,columns=['weekday'])\n",
    "train['weekday_var'] = train['date'].dt.weekday\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date'])\n",
    "test['weekday'] = test['date'].dt.weekday\n",
    "test = pd.get_dummies(test,columns=['weekday'])\n",
    "test['weekday_var'] = test['date'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['in_out'] = train['in_out'].map({'시내':0,'시외':1})\n",
    "test['in_out'] = test['in_out'].map({'시내':0,'시외':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "\n",
    "coords_jejusi = (33.500770, 126.522761) #제주시의 위도 경도\n",
    "coords_seoquipo = (33.259429, 126.558217) #서귀포시의 위도 경도\n",
    "\n",
    "\n",
    "train['dis_jejusi'] = [geopy.distance.vincenty((train['latitude'].iloc[i],train['longitude'].iloc[i]), coords_jejusi).km for i in range(len(train))]\n",
    "train['dis_seoquipo'] = [geopy.distance.vincenty((train['latitude'].iloc[i],train['longitude'].iloc[i]), coords_seoquipo).km for i in range(len(train))]\n",
    "\n",
    "test['dis_jejusi'] = [geopy.distance.vincenty((test['latitude'].iloc[i],test['longitude'].iloc[i]), coords_jejusi).km for i in range(len(test))]\n",
    "test['dis_seoquipo'] = [geopy.distance.vincenty((test['latitude'].iloc[i],test['longitude'].iloc[i]), coords_seoquipo).km for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ride_6_12'] = train[['ride_6_7','ride_7_8','ride_8_9','ride_9_10','ride_10_11','ride_11_12']].sum(axis=1)\n",
    "test['ride_6_12'] = test[['ride_6_7','ride_7_8','ride_8_9','ride_9_10','ride_10_11','ride_11_12']].sum(axis=1)\n",
    "\n",
    "train['takeoff_6_12'] = train[['takeoff_6_7','takeoff_7_8','takeoff_8_9','takeoff_9_10','takeoff_10_11','takeoff_11_12']].sum(axis=1)\n",
    "test['takeoff_6_12'] = test[['takeoff_6_7','takeoff_7_8','takeoff_8_9','takeoff_9_10','takeoff_10_11','takeoff_11_12']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "added = []\n",
    "input_var=['in_out','latitude','longitude','ride_6_7', 'ride_7_8', 'ride_8_9', \n",
    "           'ride_9_10','ride_10_11', 'ride_11_12','ride_6_12',\n",
    "           'takeoff_6_7', 'takeoff_7_8', 'takeoff_8_9','takeoff_9_10', \n",
    "           'takeoff_10_11', 'takeoff_11_12','takeoff_6_12',\n",
    "           'weekday_0', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4',\n",
    "           'weekday_5', 'weekday_6', \n",
    "           'dis_jejusi', 'dis_seoquipo']\n",
    "target=['ride_18_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "holi = ['2019-09-12','2019-09-13','2019-09-14', '2019-10-03','2019-10-09']\n",
    "wkend = ['2019-09-01','2019-09-07','2019-09-08','2019-09-14','2019-09-15',\n",
    "         '2019-09-21','2019-09-22','2019-09-28','2019-09-29',\n",
    "        '2019-10-05','2019-10-06','2019-10-12','2019-10-13']\n",
    "workday = sorted(list(set(pd.concat([train.date,test.date],axis=0).astype('str').unique()) - set(holi+wkend)))\n",
    "\n",
    "train['day_type'] =  np.where(train.date.isin(holi),1, \n",
    "                            np.where(train.date.isin(wkend),2,3))\n",
    "test['day_type'] =  np.where(test.date.isin(holi),1, \n",
    "                            np.where(test.date.isin(wkend),2,3))\n",
    "\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train,columns=['day_type'])\n",
    "test = pd.get_dummies(test,columns=['day_type'])\n",
    "\n",
    "added += [a for a in train.columns if 'day_type' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index()\n",
    "\n",
    "for aaa in ['bus_route_id','station_code','station_name']:\n",
    "    temp = all.groupby(aaa)['id'].count().reset_index().\\\n",
    "    rename(columns = {'id' : str(aaa) + '_freq'})\n",
    "\n",
    "    train = pd.merge(train,temp,how='left',on=aaa)\n",
    "    test = pd.merge(test,temp,how='left',on=aaa)\n",
    "\n",
    "    \n",
    "del all\n",
    "added += [a for a in train.columns if 'freq' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data')\n",
    "\n",
    "# 2015년 9월, 10월 의 제주도 좌표별 유동인구 공공데이터 활용\n",
    "move_18_20 = pd.read_pickle('move_18_20.pkl')\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "remove_outlier =\\\n",
    "move_18_20[~((move_18_20.x < 126.46) | ((move_18_20['x'] > 126.56) & (move_18_20['y'] > 33.5)))].reset_index(drop=True)\n",
    "\n",
    "logic1 = ((all.latitude < 33.5) & (all.latitude > 33.47)) & ((all.longitude > 126.47) & (all.longitude < 126.50))\n",
    "logic2 = ((all.latitude < 33.53) & (all.latitude > 33.48)) & ((all.longitude > 126.51) & (all.longitude < 126.54))\n",
    "logic3 = ((all.latitude < 33.26) & (all.latitude > 33.24)) & ((all.longitude > 126.55) & (all.longitude < 126.57))\n",
    "\n",
    "all['high_move'] = np.where(logic1,'1',np.where(logic2,'2',np.where(logic3,'3','0')))\n",
    "all = pd.get_dummies(all,columns=['high_move'])\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'high_move' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "logic1 = ((remove_outlier.y < 33.5) & (remove_outlier.y > 33.47)) & ((remove_outlier.x > 126.47) & (remove_outlier.x < 126.50))\n",
    "logic2 = ((remove_outlier.y < 33.53) & (remove_outlier.y > 33.48)) & ((remove_outlier.x > 126.51) & (remove_outlier.x < 126.54))\n",
    "logic3 = ((remove_outlier.y < 33.26) & (remove_outlier.y > 33.24)) & ((remove_outlier.x > 126.55) & (remove_outlier.x < 126.57))\n",
    "\n",
    "which1 = (remove_outlier[logic1].y.mean(), remove_outlier[logic1].x.mean())\n",
    "which2 = (remove_outlier[logic2].y.mean(), remove_outlier[logic2].x.mean())\n",
    "which3 = (remove_outlier[logic3].y.mean(), remove_outlier[logic3].x.mean())\n",
    "\n",
    "import geopy.distance\n",
    "all['dis_1'] = [geopy.distance.vincenty((all['latitude'].iloc[i],all['longitude'].iloc[i]), which1).km for i in range(len(all))]\n",
    "all['dis_2'] = [geopy.distance.vincenty((all['latitude'].iloc[i],all['longitude'].iloc[i]), which2).km for i in range(len(all))] \n",
    "all['dis_3']  = [geopy.distance.vincenty((all['latitude'].iloc[i],all['longitude'].iloc[i]), which3).km for i in range(len(all))]\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['dis_1','dis_2','dis_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data')\n",
    "sep = pd.read_pickle('sep_move.pkl')\n",
    "octo = pd.read_pickle('octo_move.pkl')\n",
    "\n",
    "total = pd.concat([sep,octo],axis=0)\n",
    "total[['x','y']] = round(total[['x','y']],2)\n",
    "temp = total.groupby(['x','y'])['move_18_20'].sum().reset_index().\\\n",
    "rename(columns = {'x' : 'longitude','y':'latitude'})\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "all[['latitude','longitude']] = round(all[['latitude','longitude']],2)\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on=['latitude','longitude'])\n",
    "all['move_18_20'] = all['move_18_20'].fillna(0).astype('int')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['move_18_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "for col in ['station_code','station_name','bus_route_id']:\n",
    "    temp = all.groupby([col])['ride_6_12'].agg(['mean','max','min','count']).reset_index().\\\n",
    "        rename(columns = {'mean' : col+'_'+'ride_6_12'+'_'+'mean_morning',\n",
    "                         'max' : col+'_'+'ride_6_12'+'_'+'max_morning',\n",
    "                         'min' : col+'_'+'ride_6_12'+'_'+'min_morning',\n",
    "                         'count' : col+'_'+'ride_6_12'+'_'+'count_morning'})\n",
    "    all = pd.merge(all,temp,how='left',on=col)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "for col in ['station_code','station_name','bus_route_id']:\n",
    "    temp = all.groupby([col])['takeoff_6_12'].agg(['mean','max','min','count']).reset_index().\\\n",
    "        rename(columns = {'mean' : col+'_'+'takeoff_6_12'+'_'+'mean_morning',\n",
    "                         'max' : col+'_'+'takeoff_6_12'+'_'+'max_morning',\n",
    "                         'min' : col+'_'+'takeoff_6_12'+'_'+'min_morning',\n",
    "                         'count' : col+'_'+'takeoff_6_12'+'_'+'count_morning'})\n",
    "    all = pd.merge(all,temp,how='left',on=col)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if '_morning' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "all['in_out_bus_route_id'] = all['bus_route_id'].astype('str') + all['in_out'].astype('str')\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['id'].count().to_dict()\n",
    "\n",
    "all['inout_bus_route_id_freq'] = all['in_out_bus_route_id'].map(temp)\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['ride_6_12'].agg(['mean','min','max','sum']).rename(\n",
    "columns = {\n",
    "    'mean' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'mean',\n",
    "    'min' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'min',\n",
    "    'max' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'max',\n",
    "    'sum' : 'inout_bus_route_id'+ '_' + 'ride_6_12' +'_'+'sum'\n",
    "})\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on='in_out_bus_route_id')\n",
    "\n",
    "temp = all.groupby('in_out_bus_route_id')['takeoff_6_12'].agg(['mean','min','max','sum']).rename(\n",
    "columns = {\n",
    "    'mean' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'mean',\n",
    "    'min' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'min',\n",
    "    'max' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'max',\n",
    "    'sum' : 'inout_bus_route_id'+ '_' + 'takeoff_6_12' +'_'+'sum'\n",
    "})\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on='in_out_bus_route_id')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'inout_' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "all['diff_ride_takeoff'] = all['ride_6_12'] - all['takeoff_6_12']\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'diff_ride_takeoff' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "all['bus_route_id_station_code_concat'] = str(all['station_code']) + str(all['bus_route_id'])\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data\\\\raw')\n",
    "bus = pd.read_csv('bus_bts.csv')\n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "code_which = all[['station_code','latitude','longitude']].drop_duplicates().reset_index(drop=True).rename(columns = {\n",
    "    'station_code' : 'geton_station_code',\n",
    "    'latitude' : 'geton_lat',\n",
    "    'longitude' : 'geton_long'\n",
    "})\n",
    "bus = pd.merge(bus,code_which, how='left',on='geton_station_code')\n",
    "\n",
    "code_which = all[['station_code','latitude','longitude']].drop_duplicates().reset_index(drop=True).rename(columns = {\n",
    "    'station_code' : 'getoff_station_code',\n",
    "    'latitude' : 'getoff_lat',\n",
    "    'longitude' : 'getoff_long'\n",
    "})\n",
    "bus = pd.merge(bus,code_which, how='left',on='getoff_station_code')\n",
    "\n",
    "all = pd.merge(all,pd.DataFrame({'station_code':bus[bus['geton_station_code'] == bus['getoff_station_code']].geton_station_code.unique(),\n",
    "             'same_on_off' : 1}),how='left',on='station_code') \n",
    "\n",
    "all['same_on_off'] = all['same_on_off'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "geton = []\n",
    "getoff = []\n",
    "for aa,bb,cc,dd in zip(bus['geton_lat'],bus['geton_long'],bus['getoff_lat'],bus['getoff_long']):\n",
    "    a = (aa,bb)\n",
    "    b = (cc,dd)\n",
    "    geton += [a]\n",
    "    getoff += [b]\n",
    "    \n",
    "from haversine import haversine\n",
    "\n",
    "dis = []\n",
    "for on,off in zip(geton,getoff):\n",
    "    dis += [haversine(on,off)]\n",
    "    \n",
    "bus['moving_dis'] = dis\n",
    "\n",
    "temp = bus.groupby('bus_route_id')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_bus'] = all['bus_route_id'].map(temp)\n",
    "\n",
    "temp = bus.groupby('geton_station_code')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_geton'] = all['station_code'].map(temp)\n",
    "\n",
    "temp = bus.groupby('getoff_station_code')['moving_dis'].mean().fillna(0).to_dict()\n",
    "\n",
    "all['moving_dis_per_getoff'] = all['station_code'].map(temp)\n",
    "\n",
    "all['moving_dis_per_bus'] =  all['moving_dis_per_bus'].fillna(all['moving_dis_per_bus'].median()) \n",
    "all['moving_dis_per_getoff'] = all['moving_dis_per_getoff'].fillna(all['moving_dis_per_getoff'].median()) \n",
    "all['moving_dis_per_geton'] = all['moving_dis_per_geton'].fillna(all['moving_dis_per_geton'].median()) \n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'moving_dis' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),1,0)\n",
    "temp = bus.groupby('geton_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel1'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),1,0)\n",
    "temp = bus.groupby('getoff_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel2'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),0,1)\n",
    "temp = bus.groupby('geton_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel3'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "bus['travel'] = bus['user_card_id'].map(bus['user_card_id'].value_counts()[bus['user_card_id'].value_counts()<10].to_dict())\n",
    "bus['travel'] = np.where(bus['travel'].isnull(),0,1)\n",
    "temp = bus.groupby('getoff_station_code')['travel'].sum().to_dict()\n",
    "\n",
    "all['travel4'] = all['station_code'].map(temp).fillna(0)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all, bus\n",
    "\n",
    "added += ['travel1','travel2','travel3','travel4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "temp = np.power(all[['ride_6_7','ride_7_8','ride_8_9','ride_9_10','ride_10_11','ride_11_12','takeoff_6_7','takeoff_7_8','takeoff_8_9','takeoff_9_10','takeoff_10_11','takeoff_11_12']],2)\n",
    "temp.columns = [a+'_power' for a in tuple(temp.columns)]\n",
    "\n",
    "all = pd.concat([all,temp],axis=1)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += [a for a in train.columns if 'power' in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance \n",
    "\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "jeju=(33.51411, 126.52969) # 제주 측정소 근처\n",
    "gosan=(33.29382, 126.16283) #고산 측정소 근처\n",
    "seongsan=(33.38677, 126.8802) #성산 측정소 근처\n",
    "po=(33.24616, 126.5653) #서귀포 측정소 근처\n",
    "\n",
    "t1 = [geopy.distance.vincenty( (i,j), jeju).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t2 = [geopy.distance.vincenty( (i,j), gosan).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t3 = [geopy.distance.vincenty( (i,j), seongsan).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "t4 = [geopy.distance.vincenty( (i,j), po).km for i,j in list( zip( all['latitude'],all['longitude'] )) ]\n",
    "\n",
    "all['dis_jeju']=t1\n",
    "all['dis_gosan']=t2\n",
    "all['dis_seongsan']=t3\n",
    "all['dis_po']=t4\n",
    "\n",
    "all['dist_name'] = all[['dis_jeju','dis_gosan','dis_seongsan','dis_po']].apply(lambda x: np.argmin(x),axis=1).str.slice(4,)\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['dis_jeju','dis_gosan','dis_seongsan','dis_po']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019년 9월, 10월 06~12 시의 날씨 데이터 기상청에서 가져옴.\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data')\n",
    "rain = pd.read_csv('rain.csv')[['date','dist_name','sum','std']].rename(columns={'sum':'rain_sum',\n",
    "                                              'std':'rain_std',\n",
    "                                              'max' : 'rain_max',\n",
    "                                              'min': 'rain_min'})\n",
    "temper = pd.read_csv('temper.csv')[['date','dist_name','mean','std']].rename(columns = {'mean':'temp_mean',\n",
    "                                                    'std':'temp_std',\n",
    "                                                    'max' : 'temp_max',\n",
    "                                                    'min' : 'temp_min'})\n",
    "\n",
    "train['date'] = train['date'].astype('str')\n",
    "test['date'] = test['date'].astype('str')\n",
    "\n",
    "train = pd.merge(train,rain,how='left',on=['dist_name','date'])\n",
    "train = pd.merge(train,temper,how='left',on=['dist_name','date'])\n",
    "train['temp_mean'] = train['temp_mean']/(train['temp_mean'].max())\n",
    "train['temp_std'] = train['temp_std']/(train['temp_std'].max())\n",
    "\n",
    "test = pd.merge(test,rain,how='left',on=['dist_name','date'])\n",
    "test = pd.merge(test,temper,how='left',on=['dist_name','date'])\n",
    "test['temp_mean'] = test['temp_mean']/(test['temp_mean'].max())\n",
    "test['temp_std'] = test['temp_std']/(test['temp_std'].max())\n",
    "\n",
    "added += ['rain_sum','rain_std','temp_mean','temp_std']#,'rain_max','rain_min','temp_max','temp_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "rmean = all[['date','bus_route_id','station_code','ride_6_12']].sort_values(['bus_route_id','station_code','date']).groupby(['bus_route_id','station_code'])['ride_6_12'].rolling(3).mean().reset_index()[['bus_route_id','station_code','ride_6_12']]\n",
    "rmean['date'] = all[['date','bus_route_id','station_code']].sort_values(['bus_route_id','station_code','date']).reset_index(drop=True)['date']\n",
    "all = pd.merge(all,rmean.rename(columns = {'ride_6_12' : 'r3mean_ride_6_12'}),how='left',on=['bus_route_id','station_code','date'])\n",
    "\n",
    "all['r3mean_ride_6_12'] = all[['bus_route_id','station_code','ride_6_12','r3mean_ride_6_12']].groupby(['bus_route_id','station_code']).\\\n",
    "apply(lambda x: x.fillna(x['ride_6_12'].median()))['r3mean_ride_6_12']\n",
    "\n",
    "rmean = all[['date','bus_route_id','station_code','ride_6_12']].sort_values(['bus_route_id','station_code','date']).groupby(['bus_route_id','station_code'])['ride_6_12'].rolling(5).mean().reset_index()[['bus_route_id','station_code','ride_6_12']]\n",
    "rmean['date'] = all[['date','bus_route_id','station_code']].sort_values(['bus_route_id','station_code','date']).reset_index(drop=True)['date']\n",
    "all = pd.merge(all,rmean.rename(columns = {'ride_6_12' : 'r5mean_ride_6_12'}),how='left',on=['bus_route_id','station_code','date'])\n",
    "\n",
    "all['r5mean_ride_6_12'] = all[['bus_route_id','station_code','ride_6_12','r5mean_ride_6_12']].groupby(['bus_route_id','station_code']).\\\n",
    "apply(lambda x: x.fillna(x['ride_6_12'].median()))['r5mean_ride_6_12']\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all\n",
    "\n",
    "added += ['r3mean_ride_6_12','r5mean_ride_6_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopy.distance\n",
    "os.chdir('C:\\\\Users\\\\yeonjun.in\\\\Desktop\\\\연준\\\\캐글\\\\제주도\\\\data\\\\raw')\n",
    "all = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
    "\n",
    "pop_refine = pd.read_csv('pop_refine.csv',engine='python')\n",
    "\n",
    "temp = all[['latitude','longitude']].drop_duplicates()\n",
    "\n",
    "temp = pd.DataFrame({\n",
    "    'latitude' : list(np.repeat(temp['latitude'],len(pop_refine))),\n",
    "    'longitude' : list(np.repeat(temp['longitude'],len(pop_refine))),\n",
    "    'index' : list(np.repeat(range(len(temp)),len(pop_refine))),\n",
    "    'latitude_' : list(np.tile(pop_refine['latitude'],len(temp))),\n",
    "    'longitude_' :list(np.tile(pop_refine['longitude'],len(temp))),\n",
    "    'pop' :list(np.tile(pop_refine['pop'],len(temp)))})\n",
    "\n",
    "temp['dist'] = temp.apply(lambda x: geopy.distance.vincenty((x['latitude'],x['longitude']), (x['latitude_'],x['longitude_'])).km,axis=1)\n",
    "\n",
    "temp = pd.merge(temp,temp.groupby(['latitude','longitude'])['dist'].min().reset_index(),how='right',on=['latitude','longitude','dist'])[['latitude','longitude','pop']]\n",
    "\n",
    "temp['pop'] = [int(a.replace(',','')) for a in temp['pop'] ]\n",
    "\n",
    "all = pd.merge(all,temp,how='left',on=['latitude','longitude'])\n",
    "\n",
    "all['pop_weekday'] = (all['pop'].astype('str') + all['weekday_var'].astype('str')).astype('int')\n",
    "\n",
    "train = all.loc[:(train.shape[0]-1),]\n",
    "test = all.loc[train.shape[0]:,].drop('ride_18_20',axis=1).reset_index(drop=True)\n",
    "\n",
    "del all,temp\n",
    "\n",
    "added += ['pop','pop_weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = input_var + added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_encoding_col = ['station_code','bus_route_id','station_name','bus_route_id_station_code_concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mean_encoding_col:\n",
    "    input_var += [col+'_'+'mean_target_encoding']\n",
    "    input_var += [col+'_'+'max_target_encoding']\n",
    "    input_var += [col+'_'+'min_target_encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train[target]\n",
    "\n",
    "feature_imporatnce = pd.DataFrame()\n",
    "\n",
    "NFOLDS = 6\n",
    "random_seed = sample(seed_list,1)\n",
    "\n",
    "stk = StratifiedKFold(n_splits=NFOLDS,random_state = random_seed[0],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoding(col,tr,vl,tst):\n",
    "\n",
    "    temp = tr.groupby([col])['ride_18_20'].agg(['mean','max','min']).reset_index().\\\n",
    "    rename(columns = {'mean' : col+'_'+'mean_target_encoding',\n",
    "                     'max' : col+'_'+'max_target_encoding',\n",
    "                     'min' : col+'_'+'min_target_encoding'\n",
    "                     })\n",
    "    tr_ = pd.merge(tr,temp,how='left',on= col)\n",
    "    vl_ = pd.merge(vl,temp,how='left',on= col)\n",
    "    tst_ = pd.merge(tst,temp,how='left',on= col)\n",
    "    \n",
    "    cols = [a for a in tr_.columns if 'target_encoding' in a]\n",
    "    \n",
    "    tr_[cols] = tr_[cols].fillna(0)\n",
    "    vl_[cols] = vl_[cols].fillna(0)\n",
    "    tst_[cols] = tst_[cols].fillna(0)\n",
    "\n",
    "    return tr_, vl_,tst_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "2.19871576875772\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 2\n",
      "2.220187327859567\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 3\n",
      "2.331361702075442\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 4\n",
      "2.388437173577692\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 5\n",
      "2.438631318137795\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "Fold: 6\n",
      "2.3642441683452904\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "cv score:\n",
      "2.325215824101237\n",
      "2:01:50.699081\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "time = str(start.hour)+'hr'\n",
    "minute = str(start.minute)+'min'\n",
    "\n",
    "cv_train = np.zeros(len(y_train))\n",
    "cv_pred = np.zeros(test.shape[0])\n",
    "fold_scores = []\n",
    "\n",
    "\n",
    "for fold_, (tr_index, vl_index) in enumerate(stk.split(train,train['date'])):\n",
    "    print('Fold:', fold_+1)\n",
    "  \n",
    "    x_tr, x_vl = train.iloc[tr_index], train.iloc[vl_index]\n",
    "    y_tr, y_vl = train[target].iloc[tr_index], train[target].iloc[vl_index]\n",
    "    x_tst = test.copy()\n",
    "    \n",
    "    for aaaa in mean_encoding_col:\n",
    "        x_tr,x_vl,x_tst = mean_encoding(aaaa,x_tr,x_vl,x_tst)\n",
    "    \n",
    "    tr = x_tr[input_var]\n",
    "    vl = x_vl[input_var]\n",
    "    tst = x_tst[input_var]\n",
    "    \n",
    "    rf = RandomForestRegressor(random_state=random_seed[0],n_estimators=100,criterion='mse')\n",
    "    rf.fit(tr,y_tr)\n",
    "    \n",
    "    feature_imporatnce = pd.concat([feature_imporatnce, pd.DataFrame({'feature':input_var,'importance':rf.feature_importances_})],axis=0)\n",
    "    \n",
    "    pred = rf.predict(vl)\n",
    "    \n",
    "    print(np.sqrt(mean_squared_error(y_vl,pred)))\n",
    "    cv_train[vl_index] += pred\n",
    "    cv_pred += rf.predict(tst)\n",
    "    \n",
    "    print('-'*40+'\\n\\n')\n",
    "    \n",
    "cv_pred /= NFOLDS\n",
    "\n",
    "vl_error = np.sqrt(mean_squared_error(np.array(y_train).flatten(),cv_train))\n",
    "\n",
    "print('cv score:')\n",
    "print(vl_error)\n",
    "\n",
    "sub['18~20_ride'] = cv_pred\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>r5mean_ride_6_12</td>\n",
       "      <td>0.186138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>r3mean_ride_6_12</td>\n",
       "      <td>0.116973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>station_code_max_target_encoding</td>\n",
       "      <td>0.107976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ride_6_12</td>\n",
       "      <td>0.100534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bus_route_id_mean_target_encoding</td>\n",
       "      <td>0.045989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>station_code_mean_target_encoding</td>\n",
       "      <td>0.043526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>station_name_mean_target_encoding</td>\n",
       "      <td>0.040656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>station_name_max_target_encoding</td>\n",
       "      <td>0.040159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>pop_weekday</td>\n",
       "      <td>0.023517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>temp_std</td>\n",
       "      <td>0.018739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bus_route_id_max_target_encoding</td>\n",
       "      <td>0.015041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>diff_ride_takeoff</td>\n",
       "      <td>0.014959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>day_type_3</td>\n",
       "      <td>0.013155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>temp_mean</td>\n",
       "      <td>0.012723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ride_11_12_power</td>\n",
       "      <td>0.010378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>ride_11_12</td>\n",
       "      <td>0.009857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>takeoff_6_12</td>\n",
       "      <td>0.007701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bus_route_id_ride_6_12_max_morning</td>\n",
       "      <td>0.006544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>inout_bus_route_id_ride_6_12_max</td>\n",
       "      <td>0.005993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ride_9_10</td>\n",
       "      <td>0.005512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                feature  importance\n",
       "51                     r5mean_ride_6_12    0.186138\n",
       "50                     r3mean_ride_6_12    0.116973\n",
       "68     station_code_max_target_encoding    0.107976\n",
       "58                            ride_6_12    0.100534\n",
       "2     bus_route_id_mean_target_encoding    0.045989\n",
       "69    station_code_mean_target_encoding    0.043526\n",
       "81    station_name_mean_target_encoding    0.040656\n",
       "80     station_name_max_target_encoding    0.040159\n",
       "49                          pop_weekday    0.023517\n",
       "105                            temp_std    0.018739\n",
       "1      bus_route_id_max_target_encoding    0.015041\n",
       "18                    diff_ride_takeoff    0.014959\n",
       "17                           day_type_3    0.013155\n",
       "104                           temp_mean    0.012723\n",
       "57                     ride_11_12_power    0.010378\n",
       "56                           ride_11_12    0.009857\n",
       "95                         takeoff_6_12    0.007701\n",
       "5    bus_route_id_ride_6_12_max_morning    0.006544\n",
       "34     inout_bus_route_id_ride_6_12_max    0.005993\n",
       "65                            ride_9_10    0.005512"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imporatnce.groupby(['feature'])['importance'].mean().reset_index().sort_values('importance',ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
